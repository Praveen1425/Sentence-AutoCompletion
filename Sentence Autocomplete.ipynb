{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e179d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7d2e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dataline      Play  PlayerLinenumber ActSceneLine         Player  \\\n",
      "0         1  Henry IV               NaN          NaN            NaN   \n",
      "1         2  Henry IV               NaN          NaN            NaN   \n",
      "2         3  Henry IV               NaN          NaN            NaN   \n",
      "3         4  Henry IV               1.0        1.1.1  KING HENRY IV   \n",
      "4         5  Henry IV               1.0        1.1.2  KING HENRY IV   \n",
      "\n",
      "                                          PlayerLine  \n",
      "0                                              ACT I  \n",
      "1                       SCENE I. London. The palace.  \n",
      "2  Enter KING HENRY, LORD JOHN OF LANCASTER, the ...  \n",
      "3             So shaken as we are, so wan with care,  \n",
      "4         Find we a time for frighted peace to pant,  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Shakespeare_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b36a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACT I',\n",
       " 'SCENE I. London. The palace.',\n",
       " 'Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others',\n",
       " 'So shaken as we are, so wan with care,',\n",
       " 'Find we a time for frighted peace to pant,']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting text from the data\n",
    "text = []\n",
    "for i in data['PlayerLine']:\n",
    "    text.append(i)\n",
    "    \n",
    "# lets see how the text is looking \n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70b2584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['act i',\n",
       " 'scene i london the palace',\n",
       " 'enter king henry lord john of lancaster the earl of westmoreland sir walter blunt and others',\n",
       " 'so shaken as we are so wan with care',\n",
       " 'find we a time for frighted peace to pant']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Cleaning\n",
    "def clean_text(text):\n",
    "    # removing special characters like @, #, $, etc\n",
    "    pattern = re.compile('[^a-zA-z0-9\\s]')\n",
    "    text = re.sub(pattern,'',text)\n",
    "\n",
    "    # removing digits\n",
    "    pattern = re.compile('\\d+')\n",
    "    text = re.sub(pattern,'',text)\n",
    "\n",
    "    # converting text to lower case\n",
    "    text = text.lower()\n",
    "    return text\n",
    "  \n",
    "texts = []\n",
    "for t in text:\n",
    "    new_text = clean_text(t)\n",
    "    texts.append(new_text)\n",
    "    \n",
    "# cleaned text\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054a4f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text -->> act i\n",
      "Embedding -->> [455, 4]\n",
      "Maximum Sequence Length -->> 54\n",
      "Text Sequence -->>\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 455   4]\n",
      "Text Sequence Shape -->> (10000, 54)\n"
     ]
    }
   ],
   "source": [
    "# lets take first 10000 words for the model training\n",
    "texts = texts[:10000]\n",
    "\n",
    "# using tensorflow tokenizer and\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# generating text sequences, i.e. encoding the text \n",
    "text_sequences = tokenizer.texts_to_sequences(texts) # Remove np.array() \n",
    "print('Text -->>',texts[0])\n",
    "print('Embedding -->>',text_sequences[0])\n",
    "\n",
    "# padding the sequences \n",
    "Max_Sequence_Len = max([len(x) for x in text_sequences])\n",
    "text_sequences = pad_sequences(text_sequences, \n",
    "                               maxlen = Max_Sequence_Len, padding='pre') # pad_sequences expects a list\n",
    "\n",
    "print('Maximum Sequence Length -->>',Max_Sequence_Len)\n",
    "print('Text Sequence -->>\\n',text_sequences[0])\n",
    "print('Text Sequence Shape -->>',text_sequences.shape)\n",
    "\n",
    "# This code is modified by Susobhan Akhuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6daa5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Input : [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 455]\n",
      "First Target : 4\n",
      "Total Number of Words: 7865\n",
      "Input Shape : (10000, 53)\n",
      "Target Shape : (10000, 7865)\n"
     ]
    }
   ],
   "source": [
    "# getting X and y from the data\n",
    "X, y = text_sequences[:, :-1], text_sequences[:,-1]\n",
    "print('First Input :',X[0])\n",
    "print('First Target :',y[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# using one hot encoding on y \n",
    "total_words = len(word_index) + 1\n",
    "print('Total Number of Words:',total_words)\n",
    "\n",
    "y = to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# printing X and y shapes\n",
    "print('Input Shape :',X.shape)\n",
    "print('Target Shape :',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c506164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"LSTM_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(name=\"LSTM_Model\")\n",
    "\n",
    "# adding embedding layer\n",
    "model.add(Embedding(total_words, \n",
    "                    Max_Sequence_Len-1, \n",
    "                    input_length=Max_Sequence_Len-1))\n",
    "\n",
    "# adding a LSTM layer \n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#adding the final output activation with activation function of softmax\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# printing model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 734ms/step - accuracy: 0.0111 - loss: 8.1673\n",
      "Epoch 2/50\n",
      "\u001b[1m 50/313\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 753ms/step - accuracy: 0.0198 - loss: 7.2115"
     ]
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training the LSTM model\n",
    "history = model.fit(X, y,\n",
    "                       epochs=50,\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoCompletations(text, model):\n",
    "    # Tokenization and Text vectorization\n",
    "    text_sequences = np.array(tokenizer.texts_to_sequences([text]))\n",
    "    # Pre-padding\n",
    "    testing = pad_sequences(text_sequences, maxlen = Max_Sequence_Len-1, padding='pre')\n",
    "    # Prediction\n",
    "    y_pred_test = np.argmax(model.predict(testing,verbose=0))\n",
    "    \n",
    "    predicted_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == y_pred_test:\n",
    "            predicted_word = word\n",
    "            break\n",
    "    text += \" \" + predicted_word + '.'\n",
    "    return text \n",
    "  \n",
    "complete_sentence = autoCompletations('I have seen this', model)\n",
    "complete_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf04825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text, new_words):\n",
    "    for _ in range(new_words):\n",
    "        text = autoCompletations(text, model)[:-1]\n",
    "    return text \n",
    "  \n",
    "generated_text = generate_text('I have seen', 5)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b557f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model.save('sentence_completion.h5')\n",
    "\n",
    "# saving the tokenizer\n",
    "filename = 'tokenizer.pkl'\n",
    "pickle.dump(tokenizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff3665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
